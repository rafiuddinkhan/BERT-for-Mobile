{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "6 - Transformers - DistilBERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/BERT-for-Mobile/blob/master/DistilBERT_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhcSBOJvj54"
      },
      "source": [
        "*This notebook is copied from [here](https://github.com/dipanjanS/deep_transfer_learning_nlp_dhs2019/blob/master/notebooks/6%20-%20Transformers%20-%20DistilBERT.ipynb). The purpose of this notebook is to show how to convert a custom DistilBERT-based model to TensorFlow Lite.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KVYvYX-lgd3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK6zJE1algeP"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMUSaRgElgeT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "print(\"TF Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU95IkmalgeX"
      },
      "source": [
        "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
        "dataset['sentiment'] = [1 if record == 'positive' else 0 for record in dataset['sentiment']]\n",
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5PIDFJ1lgec"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVNAbNntlgel"
      },
      "source": [
        "reviews = dataset['review'].values\n",
        "sentiments = dataset['sentiment'].values\n",
        "\n",
        "train_reviews = reviews[:5000]\n",
        "val_reviews = reviews [5000:10000]\n",
        "test_reviews = reviews[10000:]\n",
        "\n",
        "\n",
        "\n",
        "train_sentiments = sentiments[:5000]\n",
        "val_sentiments = sentiments [5000:10000]\n",
        "test_sentiments = sentiments[10000:]\n",
        "\n",
        "train_reviews.shape, val_reviews.shape, test_reviews.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k68KgD7ulgep"
      },
      "source": [
        "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHScGsj8lgez"
      },
      "source": [
        "import tqdm\n",
        "\n",
        "def create_bert_input_features(tokenizer, docs, max_seq_length):\n",
        "    \n",
        "    all_ids, all_masks = [], []\n",
        "    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n",
        "        tokens = tokenizer.tokenize(doc)\n",
        "        if len(tokens) > max_seq_length-2:\n",
        "            tokens = tokens[0 : (max_seq_length-2)]\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        masks = [1] * len(ids)\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(ids) < max_seq_length:\n",
        "            ids.append(0)\n",
        "            masks.append(0)\n",
        "        all_ids.append(ids)\n",
        "        all_masks.append(masks)\n",
        "    encoded = np.array([all_ids, all_masks])\n",
        "    return encoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtqmuFyFlge6"
      },
      "source": [
        "MAX_SEQ_LENGTH = 500\n",
        "\n",
        "inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\n",
        "inp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\n",
        "inputs = [inp_id, inp_mask]\n",
        "\n",
        "hidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\n",
        "pooled_output = hidden_state[:, 0]    \n",
        "dense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\n",
        "drop1 = tf.keras.layers.Dropout(0.25)(dense1)\n",
        "dense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\n",
        "drop2 = tf.keras.layers.Dropout(0.25)(dense2)\n",
        "output = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "model.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-5, \n",
        "                                           epsilon=1e-08), \n",
        "              loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpDBzyDTs4HI"
      },
      "source": [
        "tf.keras.utils.plot_model(model, \"model.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j1gzPXPlge-"
      },
      "source": [
        "train_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_reviews, \n",
        "                                                                      max_seq_length=MAX_SEQ_LENGTH)\n",
        "val_features_ids, val_features_masks = create_bert_input_features(tokenizer, val_reviews, \n",
        "                                                                  max_seq_length=MAX_SEQ_LENGTH)\n",
        "#test_features = create_bert_input_features(tokenizer, test_reviews, max_seq_length=MAX_SEQ_LENGTH)\n",
        "print('Train Features:', train_features_ids.shape, train_features_masks.shape)\n",
        "print('Val Features:', val_features_ids.shape, val_features_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnTMVebJlgfE"
      },
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                      patience=1,\n",
        "                                      restore_best_weights=True,\n",
        "                                      verbose=1)\n",
        "model.fit([train_features_ids, \n",
        "           train_features_masks], train_sentiments, \n",
        "          validation_data=([val_features_ids, \n",
        "                            val_features_masks], val_sentiments),\n",
        "          epochs=3, \n",
        "          batch_size=20, \n",
        "          shuffle=True,\n",
        "          callbacks=[es],\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5TdTdbdlgfH"
      },
      "source": [
        "model.save_weights('distillbert_ft_wts.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyx7mAt6lgfK"
      },
      "source": [
        "test_features_ids, test_features_masks = create_bert_input_features(tokenizer, test_reviews, \n",
        "                                                                    max_seq_length=MAX_SEQ_LENGTH)\n",
        "print('Test Features:', test_features_ids.shape, test_features_masks.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZufUk5gjlgfO"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "predictions = [1 if pr > 0.5 else 0 \n",
        "                   for pr in model.predict([test_features_ids, \n",
        "                                            test_features_masks], batch_size=200, verbose=0).ravel()]\n",
        "\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy_score(test_sentiments, predictions)*100))\n",
        "print(classification_report(test_sentiments, predictions))\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBedtQFq0Ha"
      },
      "source": [
        "Reference - https://github.com/huggingface/tflite-android-transformers/blob/master/models_generation/distilbert.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFfpNh7urdIC"
      },
      "source": [
        "### Dynamic-range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTofM0uXlgfT"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "open(\"distilbert_imdb_tflite.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRCR-gLgrYHS"
      },
      "source": [
        "!ls -lh distilbert_imdb_tflite.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcw2xcRLrzTP"
      },
      "source": [
        "### Float16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1MEg639rgWU"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "open(\"distilbert_imdb_tflite_fp16.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip8xKKPFrmUb"
      },
      "source": [
        "!ls -lh distilbert_imdb_tflite_fp16.tflite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_LNbd7ksIss"
      },
      "source": [
        "### Integer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhvLEwP_vKSU"
      },
      "source": [
        "# model.load_weights(\"distillbert_ft_wts.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-d9RNsmsJ_j"
      },
      "source": [
        "# ==============Representative dataset====================\n",
        "train_features_ids = train_features_ids.astype(np.int32)\n",
        "train_features_masks = train_features_masks.astype(np.int32)\n",
        "train_tf_dataset = tf.data.Dataset.from_tensor_slices((train_features_ids, \n",
        "    train_features_masks))\n",
        "\n",
        "def representative_dataset_gen():\n",
        "    for feature_id, feature_mask in train_tf_dataset.take(10):\n",
        "        yield [feature_id, feature_mask]\n",
        "\n",
        "# ==============Conversion====================\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, \n",
        "                                       tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "open(\"distilbert_imdb_tflite_int.tflite\", \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3F52NLg-0gi"
      },
      "source": [
        "***Currently, integer quantization fails.***"
      ]
    }
  ]
}